# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

This is a LangGraph-based chatbot agent deployed on AWS Bedrock AgentCore. The agent uses Claude Haiku for conversational AI, integrates with Tavily and Wikipedia for web search capabilities, and leverages AWS AgentCore Memory for both short-term (conversational context) and long-term (user preferences/facts) memory persistence.

## Development Setup

```bash
# Install uv package manager
pip install uv

# Create and activate virtual environment
uv venv
source .venv/bin/activate

# Install dependencies
uv pip install -r requirements.txt
```

## Testing

Run the test suite locally (tests single-turn, multi-turn, and memory integration):

```bash
python test_agent.py
```

Note: `test_agent.py` imports from `langgraph_agent_web_search` which should be updated to import from `agent` if testing locally.

## Deployment Commands

```bash
# Configure agent deployment settings (one-time setup)
agentcore configure

# Deploy agent to AWS Bedrock AgentCore
agentcore launch
```

During `agentcore configure`, select:
- Entrypoint: `agent.py`
- Agent name: `agent`
- Dependency file: `requirements.txt`
- Deployment type: Container
- Auto-create execution role and ECR repository
- Default IAM authorization and request headers

## Architecture

### Core Components

**agent.py** - Main agent implementation with three key layers:

1. **LangGraph State Machine**: Defines a simple graph with two nodes:
   - `chatbot` node: Invokes Claude Haiku LLM with tools bound
   - `tools` node: Executes tool calls (Tavily search, Wikipedia)
   - Uses `tools_condition` to route between chatbot and tools based on whether the LLM requests tool use

2. **State Persistence (Checkpointing)**: Uses `AgentCoreMemorySaver` from `langgraph-checkpoint-aws` to persist conversation state across turns. The checkpointer is configured with:
   - `thread_id`: Maps to session_id from the payload
   - `actor_id`: User identifier from the payload
   - This enables multi-turn conversations where the agent remembers previous exchanges within a session

3. **AgentCore Memory Integration**: Two memory types via `MemorySessionManager`:
   - **Short-term memory**: Automatic conversation history tracking (handled by checkpointer)
   - **Long-term memory**: Extracted facts/preferences saved via `session.add_turns()` and retrieved via `session.search_long_term_memories()` with semantic search
   - Long-term memories are namespaced by actor_id: `/users/{actor_id}`

**tools.py** - Tool initialization module:
- `load_tavily_api_key()`: Loads Tavily API key from environment, config.py, .env, or AWS Secrets Manager (in priority order)
- `load_search_tools()`: Initializes Tavily and Wikipedia search tools, gracefully degrading if dependencies are unavailable

### Agent Invocation Flow

1. Extract `session_id`, `actor_id`, and `prompt` from payload
2. Query long-term memories using the prompt as search query
3. Prepend relevant memories as system message context
4. Invoke LangGraph state machine with session config
5. Save conversation turn (user + assistant messages) to AgentCore Memory for future retrieval
6. Return response with session metadata

### Environment Variables

- `AGENTCORE_MEMORY_ID`: AgentCore Memory ID (falls back to hardcoded default)
- `AWS_REGION`: AWS region for AgentCore services (default: us-east-1)
- `TAVILY_API_KEY`: API key for Tavily search tool

### Configuration Files

- `.bedrock_agentcore.yaml`: AgentCore deployment configuration (auto-generated by `agentcore configure`)
- `requirements.txt`: Python dependencies
- `.dockerignore`: Files excluded from container image

## Key Architectural Patterns

**Graceful Degradation**: The agent initializes checkpointer and memory manager in try/except blocks, allowing it to run without persistence if AWS resources are unavailable (useful for local development).

**Session Management**: `session_id` enables conversation persistence across multiple invocations. Different users (actor_id) can have separate sessions and separate long-term memory namespaces.

**Tool Binding**: Tools are bound to the LLM via `llm.bind_tools(tools)`, allowing Claude to request tool calls in its responses. LangGraph's `tools_condition` automatically routes to the tools node when tool calls are requested.

**Memory Context Injection**: Long-term memories are retrieved via semantic search against the user's prompt and injected as a system message before LLM invocation, providing relevant historical context.
